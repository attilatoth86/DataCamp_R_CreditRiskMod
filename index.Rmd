---
title: "Credit Risk Modeling in R"
subtitle: "DataCamp"
author: "Attila Toth"
runtime: shiny
output: 
  html_document:
    mathjax: "http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    theme: united
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(gmodels)
library(knitr)

options(scipen=999)
```

# 1. Introduction and data preparation

This chapter begins with a general introduction to credit risk models, will explore a real-life data set, then preprocess the data set such that it's in the appropriate format before applying the credit risk models.

## 1.1 Credit Risk

Credit risk is the risk of default on a debt that may arise from a borrower failing to make required payments. In finance, default is the failure to meet the legal obligations of a loan, i.e. failing to make recurring payments, or pay back priciple of a security on maturity. Within a regular loan contract, the lender provides fund for the borrower upfront that the borrower has to pay back over time according to an agreed schedule most cases together with interest in addition. Certainly, there is a risk associated to these transactions as the borrower may not be able to reimburse the provided fund fully. These events cause losses for the lenders, mainly for banks. The concept of expected loss is composed by three elements:

 - **Probability of default** (PD): the probability that the borrower will fail to make a full repayment to the lender
 - **Exposure at default** (EAD): expected value of the loan at the time of default (similarly: the amount of the loan that still needs to be repayed at the time of default)
 - **Loss given default** (LGD): amount of loss given default (percentage of EAD)

$$\textbf{E}\text{xpected }\textbf{L}\text{oss} = \textbf{P}\text{robability of }\textbf{D}\text{efault} * \textbf{E}\text{xposure }\textbf{A}\text{t }\textbf{D}\text{efault} * \textbf{L}\text{oss }\textbf{G}\text{iven }\textbf{D}\text{efault}$$

The area of credit risk modeling is all about the event of loan default, therefore the determination of default probability is crucial. In this activity banks usually use past default patterns to predict default probabilities of future customers and loan transactions. The following pieces of information are used for the examination:

 - **application information**: descriptive information about the borrower (incl. income, age, etc)
 - **behavioral information**: past behaviours of customers (incl. account transaction and balance history)

Useful reads:

 - <https://en.wikipedia.org/wiki/Credit_risk>
 - <https://en.wikipedia.org/wiki/Default_(finance)>

## 1.2 Sourcing data

In the beginning of the course we had been provided with data set, sourced from the following location: <https://assets.datacamp.com/course/credit-risk-modeling-in-r/loan_data_ch1.rds> We will be examining the dataset `loan_data` throughout the exercises in this course.

## 1.3 Dataset Overview

```{r load in data}
loan_data <- readRDS("loan_data_ch1.rds")
```

As always, it is recommended to take a quick first look at our data. With using `str()`, we can get an idea about the data structure. This case it tells us the following:

```{r structure of the dataset}
str(loan_data)
```

Another view about the data if we use `head()` for displaying the first few rows.

```{r retrieve the first few rows}
kable(head(loan_data))
```

We had been informed about the content of the columns in the course but without that the columns were anyway very intuitively named.

|Variable Type|Variable|Description|
|-------------|--------|-------------------------------------------------------|
|Response Variable|loan_status|1 = loan defaulted, 0 = loan not defaulted|
|Explanatory Variables|loan_amt|Amount of the loan|
||int_rate|Interest rate applied on the loan|
||grade|Buro score of the customer; A-G highest-lowest class of creditworthiness. This reflects the credit history of the individual (behavioural variable)|
||emp_legth|Length of employment|
||home_ownership|Home ownership status|
||annual_inc|Annual income|
||age|Age of the customer|

After downloading and loading in `loan_data`, we are interested about the defaulted loans in the data set. We want to get an idea of the number, and percentage of defaults. Defaults are usually rare, so we always want to check what the proportion of defaults is in the examined dataset. The `gmodels::CrossTable()` function is very useful here.

```{r CrossTable(loan_status)}
CrossTable(loan_data$loan_status)
```

To learn more about variable structures and spot unexpected tendencies in the data, we should examine the relationship between `loan_status` and certain factor variables. For example, we would expect that the proportion of defaults in the group of customers with worse grades (worst credit rating score) is substantially higher than the proportion of defaults in the grade A group (best credit rating score).

```{r CrossTable() other variables}
CrossTable(loan_data$grade, loan_data$loan_status,
           prop.r = T,
           prop.c = F,
           prop.t = F,
           prop.chisq = F
           )
```

The table above tells us, the proportion of defaults increases when the credit rating moves from A to G.

## 1.4 Visuals

The best way to explore continuous variables, to identify potential outliers or unexpected data structures is always some sort of quick visualization that can provide some early information before any further processing. So I just started off with simply plotting all variables in the dataset.

All of the plots had showed fairly expected results (so I excluded them from this documentation), except for the following two:

```{r simple plots, fig.align='center'}
renderPlot({
par(mfrow=c(1,2))

plot(loan_data$annual_inc/1000, ylab = "Annual Income (k $)")
points(which(loan_data$annual_inc>5000000),
       loan_data$annual_inc[loan_data$annual_inc>5000000]/1000, 
       col = "red")

plot(loan_data$age, ylab = "Age")
points(which(loan_data$age>100),
       loan_data$age[loan_data$age>100], 
       col = "red")
})
```

## 1.5 Outliers

Apparently, we encountered some outliers above around the age of **140** (!!) and annual income of **$6M**. Without having significant expertise in a particular business domain, a generally accepted way, a rule of thumb, to introduce outlier thresholds is to filter out data points that exceed the boundaries of interquartile range, the first quartile (Q1) and the third quartile (Q3), with more than 1.5 times the interquartile range (IQR) itself, i.e. values greater than Q3 + 1.5 x IQR or lower than Q1 - 1.5 x IQR.

```{r threshold calculation}
OL_age_threshold_low <- quantile(loan_data$age, 0.25)-1.5*IQR(loan_data$age)
OL_age_threshold_hi <- quantile(loan_data$age, 0.75)+1.5*IQR(loan_data$age)

OL_anninc_threshold_low <- quantile(loan_data$annual_inc, 0.25)-1.5*IQR(loan_data$annual_inc)
OL_anninc_threshold_hi <- quantile(loan_data$annual_inc, 0.75)+1.5*IQR(loan_data$annual_inc)
```

|Variable|Outlier Boundaries||Number of Observations Outer Boundaries||
|---|---|---|---|---|
||Lower|Upper|Lower|Upper|
|age|`r OL_age_threshold_low`|`r OL_age_threshold_hi`|`r length(which(loan_data$age<OL_age_threshold_low))`|`r length(which(loan_data$age>OL_age_threshold_hi))`|
|annual_inc|`r OL_anninc_threshold_low`|`r OL_anninc_threshold_hi`|`r length(which(loan_data$annual_inc<OL_anninc_threshold_low))`|`r length(which(loan_data$annual_inc>OL_anninc_threshold_hi))`|

Luckily, our dataset was more or less tidy therefore no observation occured below the lower boundaries that actually makes sense. Outer the upper boundaries we could see significant number of observations. The thresholds set for age and annual income variables, using the general outlier threshold rule, are sort of strict, given that we might assume by nature that customers above 40 years and having over $140k annual still valid observations and want to include them in our model. Therefore I decided to remove only those two extreme outliers (annual income above $6M and age above 130) from the dataset. (This happened to be one single observation.)

```{r remove extreme outliers}
OL_idx <- which(loan_data$annual_inc>5000000 | loan_data$age>130)
loan_data_nonOL <- loan_data[-OL_idx,]
```

My adjusted dataset showed the following distributions:

```{r adj histograms, fig.align='center'}
renderPlot(height = 900, {
par(mfrow=c(4,2))
hist(loan_data_nonOL$loan_amnt, main = "", 
     xlab = "Loan Amount", ylab="")
hist(loan_data_nonOL$int_rate, main = "", 
     xlab = "Interest Rate Value", ylab="")
plot(loan_data_nonOL$grade, main = "", 
     xlab = "Grade")
hist(loan_data_nonOL$emp_length, main = "", 
     xlab = "Employment Length (in years)", ylab="")
plot(loan_data_nonOL$home_ownership, main = "", 
     xlab = "Home Ownership Status")
hist(loan_data_nonOL$annual_inc, main = "",
     breaks = sqrt(nrow(loan_data_nonOL)),
     xlab = "Annual Income", ylab="")
hist(loan_data_nonOL$age, main = "", 
     xlab = "Age", ylab="")
})
```

## 1.6 Missing Data

In case of missing data, the following strategies can be considered:

 - **Deletion**: to delete observation or the entire variable. If there are only few observations having missing values, observations are to be deleted; in case of large number of missing value in a column, the deletion of the variable should be considered.
 - **Replacement**: replacement with some central value, e.g. median imputation
 - **Keep**: many models can not deal with missing values so eventually they just ignore (delete) those observations. To be on the safe side, we can perform the so-called coarse classification that transforms continuous variable into categorical providing us the opportunity to place missing values into an arbitrary bin.

|Strategy|Continuous Variable|Categorical Variable|
|---|---|---|
|**Delete**|delete rows/columns|delete rows/columns|
|**Replace**|median imputation|using the most frequent category|
|**Keep**|keep as NA/coarse classification|create "missing" category|

Using `summary()` function on our dataset the number of missing values can be easily identified.

```{r missing data}
summary(loan_data_nonOL)
```

We encountered missing values in `emp_length` and `int_rate` variables. I created three variants of the original dataset using the three strategies explained above.

First, the simpliest solution is the deletion of those records that carry NA values either in `emp_length` or in `int_rate` variables.

```{r delete NA records}
NA_idx_del <- which(is.na(loan_data_nonOL$emp_length) | is.na(loan_data_nonOL$int_rate))
loan_data_nonOL_delNA <- loan_data_nonOL[-NA_idx_del,]
```

In the second approach, I replaced NAs with the medians of the respective variables.

```{r median imputation}
median_int_rate <- median(loan_data_nonOL$int_rate, na.rm = T)
median_emp_length <- median(loan_data_nonOL$emp_length, na.rm = T)

loan_data_nonOL_med <- loan_data_nonOL

loan_data_nonOL_med[is.na(loan_data_nonOL_med$int_rate),"int_rate"] <- median_int_rate
loan_data_nonOL_med[is.na(loan_data_nonOL_med$emp_length),"emp_length"] <- median_emp_length
```

Finally, the third option is to keep NAs in a special way that model will be able to use them. Given that both `int_rate` and `emp_length` variables are continuous, the coarse classification will be the technique I use.

I categorized the variables according to the followings:

 - `emp_length`: 0-1, 1-3, 3-5, 5-10, 10+ and **"Missing"**
 - `int_rate`: 0-8, 8-11, 11-13.5, 13.5+ and **"Missing"**

In order to avoid any potential information loss, I separated the categorization into new variables, `emp_length` -> `emp_length_cat` and `int_rate` -> `ir_cat`

```{r coarse classification}
loan_data_nonOL_keepNA <- loan_data_nonOL # create a copy of the dataset

loan_data_nonOL_keepNA$emp_length_cat <- rep(NA, length(loan_data_nonOL_keepNA$emp_length))
loan_data_nonOL_keepNA$ir_cat <- rep(NA, length(loan_data_nonOL_keepNA$int_rate))

loan_data_nonOL_keepNA$emp_length_cat[which(loan_data_nonOL_keepNA$emp_length<=1)] <- "0-1"
loan_data_nonOL_keepNA$emp_length_cat[which(loan_data_nonOL_keepNA$emp_length>1 & loan_data_nonOL_keepNA$emp_length<=3)] <- "1-3"
loan_data_nonOL_keepNA$emp_length_cat[which(loan_data_nonOL_keepNA$emp_length>3 & loan_data_nonOL_keepNA$emp_length<=5)] <- "3-5"
loan_data_nonOL_keepNA$emp_length_cat[which(loan_data_nonOL_keepNA$emp_length>5 & loan_data_nonOL_keepNA$emp_length<=10)] <- "5-10"
loan_data_nonOL_keepNA$emp_length_cat[which(loan_data_nonOL_keepNA$emp_length>10)] <- "10+"
loan_data_nonOL_keepNA$emp_length_cat[which(is.na(loan_data_nonOL_keepNA$emp_length))] <- "Missing"

loan_data_nonOL_keepNA$ir_cat[which(loan_data_nonOL_keepNA$int_rate<=8)] <- "0-8"
loan_data_nonOL_keepNA$ir_cat[which(loan_data_nonOL_keepNA$int_rate>8 & loan_data_nonOL_keepNA$int_rate<=11)] <- "8-11"
loan_data_nonOL_keepNA$ir_cat[which(loan_data_nonOL_keepNA$int_rate>11 & loan_data_nonOL_keepNA$int_rate<=13.5)] <- "11-13.5"
loan_data_nonOL_keepNA$ir_cat[which(loan_data_nonOL_keepNA$int_rate>13.5)] <- "13.5+"
loan_data_nonOL_keepNA$ir_cat[which(is.na(loan_data_nonOL_keepNA$int_rate))] <- "Missing"

loan_data_nonOL_keepNA$emp_length_cat <- as.factor(loan_data_nonOL_keepNA$emp_length_cat)
loan_data_nonOL_keepNA$ir_cat <- as.factor(loan_data_nonOL_keepNA$ir_cat)
```

### 1.7 Train - Test Split

In order to validate our model at a later point in time, we have to separate our dataset into training and testing sections before we start model build. We build our model, train our algorithm, on training set and verify it on the "unseen" testing set to get a realistic view about its performance.

We perform this separation usually with random sampling.

```{r training-testing set separation}
loan_data_proc <- loan_data_nonOL_keepNA
loan_data_proc$emp_length <- NULL
loan_data_proc$int_rate <- NULL
loan_data_proc$loan_status <- as.factor(loan_data_proc$loan_status)

set.seed(123)
train_idx <- sample(1:nrow(loan_data_proc),2/3*nrow(loan_data_proc))

loan_data_train <- loan_data_proc[train_idx, ]
loan_data_test <- loan_data_proc[-train_idx, ]
```

Let's have a look at the structure of the training set.

```{r summary of training set}
str(loan_data_train)
```

# 2. Logistic Regression

Logistic regression models are similar in many ways to linear regression except for that the output value of these models is between 0 and 1. That is why logistic regression models can be good for purposes where probability estimation is required since probability values fall by definition between 0 and 1.

In our case, we are looking for the following value, the probability of `loan_status` being 1 (loan default) given various explaining variable values:

$$P(\textrm{loan_status} = 1 | x_1,x_2,...,x_m) = \frac{1}{1 + e^{-(\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_mx_m)}}$$

In this equation gives us the linear predictor, $\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_mx_m$, the combination of parameters and variables. To fit logistic regression model, we can use the generalized linear model function, `glm()`, by instructing the general function with a specific attribute, i.e. `family = "binomial"`.

```{r fitting logistic regression}
log_model <- glm(loan_status ~ age, family = "binomial", data = loan_data_train)
log_model
```

`(Intercept)` is the estimate for $\beta_0$, values under various variables are estimates for $\beta_1$, $\beta_2$,...,$\beta_m$.

Given the following changes on the previous equation,

$$P(\textrm{loan_status} = 1 | x_1,x_2,...,x_m) = \frac{1}{1 + e^{-(\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_mx_m)}} = \frac{e^{\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_mx_m}}{1 + e^{\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_mx_m}}$$

$$P(\textrm{loan_status} = 0 | x_1,x_2,...,x_m) = 1 - \frac{e^{\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_mx_m}}{1 + e^{\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_mx_m}} = \frac{1}{1 + e^{\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_mx_m}}$$

The probability of default divided by the probability of non-default gives the odds in favor of default that equals the exponential function of the linear predictor:

$$\frac{P(\textrm{loan_status} = 1 | x_1,x_2,...,x_m)}{P(\textrm{loan_status} = 0 | x_1,x_2,...,x_m)} = e^{\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_mx_m}$$

This feature helps us a lot in the interpretation of our model.

## 2.1 Interpretation

If the value of $x_j$ goes up by 1, the odds will be multiplied by $e^{\beta_j}$. Note, this means 

 - if $\beta_j < 0$ then $e^{\beta_j} < 1$, so the odds will be multiplied by a value lower than 1, odds will decrease
 - if $\beta_j > 0$ then $e^{\beta_j} > 1$, so the odds will be multiplied by a value higher than 1, odds will increase

In my simple model above it means, if `age` goes up by 1 year the odds for default are multiplied by $e^{`r log_model[["coefficients"]]["age"]`}$ (`r exp(log_model[["coefficients"]]["age"])`). In summary, one extra age lowers the probability of default by nearly 1%.

The interpretation of a single parameter still holds when including several variables in a model, along with the assumption of keeping the other variables unchanged.

### 2.1.1 Interpretation of coefficients of categorical variables

When we include a categorical variable in a logistic regression model, we will get a parameter estimate for all but on of its categories. This category for which no parameter estimate is given is called **reference category**. The parameter for each of the other categories represents the odds ratio in favor of a loan default between the category of interest and the reference category.

```{r}
log_model_cat <- glm(loan_status ~ ir_cat, family = "binomial", data = loan_data_train)
log_model_cat
```

### 2.1.2 Model evaluation

In order to evaluate models there are a number of things to be aware of.

 - **Parameter values** a.k.a. coefficients
 - **Statistical significance of parameter estimates** above. The significance of a parameter is often refered to as a p-value or in R: `Pr(>|t|)`. When a parameter is not significant, it cannot be assured that this parameter is significantly different from 0. Statistical significance is important, in general, it only makes sense to interpret the effects for significant parameters.

```{r}
log_model_multi <- glm(loan_status ~ age + ir_cat + grade + loan_amnt + annual_inc, data = loan_data_train, family = "binomial")

summary(log_model_multi)
```

## 2.2 Prediction

After having created the model, `predict()` function provides us the functionality to make predictions. For demonstration purposes, I quickly created a small logistic model, adding `age` and `ir_cat` variables as predictors to estimate loan default probability.

```{r}
log_model_small <- glm(loan_status ~ age * ir_cat, family = "binomial", data = loan_data_train)
prediction_all_small <- predict(log_model_small, loan_data_test, type = "response")
```

Note: `predict()` function by default returns with the linear predictor for generalized linear models so in order to get the probability, we have to instruct the function with the `type = "response"` argument.

After having obtained all the predictions for the test set elements, it is useful to get an initial idea of how good the model is at discriminating by looking at the `range()` of predicted probabilities. A small range might mean that predictions for the test set cases do not lie far apart, and therefore the model might not be very good at discriminating good from bad customers.

## 2.3 Prediction Evaluation

For evaluation of models, we can make use of confusion matrix. Confusion matrices compare the actual response variable values to the predicted values in a tabular format. They are contigency tables of correct and incorrect classifications. (Correct classifications are at the diagonal, often called as True-Positives, True-Negatives.)

Confusion Matrix measures:

$Accuracy = \frac{True Negatives + True Positives}{Total Classifications}$ = proportion of correctly classified cases

$Sensitivity = \frac{True Positives}{True Positives + False Negatives}$ = proportion of correctly classified positive outcomes

$Specificity = \frac{True Negatives}{True Negatives + False Positives}$ = proportion of correctly classified negative outcomes

The problem we are facing in this case is that `loan_status` variable carries 0s and 1s while predicted probability of defaults are values between 0 and 1. In order to circumvent this gap, we have to introduce the term of cutoff or threshold value. If a predicted probability is above the threshold we will flag the observation as default otherwise as non-default.

I created a new model that includes all variables of the `loan_data` and using this model generated predictions:

```{r}
log_model_full <- glm(loan_status ~ ., family = "binomial", data = loan_data_train)
prediction_all_full <- predict(log_model_full, loan_data_test, type = "response")
range(prediction_all_full)
```

Predicted default probabilities range from almost 0 to 0.55. The obvious choice for a cutoff value would be 0.5. Let's see what we get then:

```{r}
pred_co_05 <- ifelse(prediction_all_full<0.5,0,1)
cm_05 <- table(loan_data_test$loan_status,pred_co_05)
cm_05
```

With the cutoff value of 0.5 my prediction shows the following measures:

 - Accuracy: `r sum(diag(cm_05))/sum(cm_05)`
 - Sensitivity: `r cm_05[2,2]/sum(cm_05[2,])`
 - Specificity: `r cm_05[1,1]/sum(cm_05[1,])`

Setting a lower, 0.15, threshold will result the following:

```{r}
pred_co_015 <- ifelse(prediction_all_full<0.15,0,1)
cm_015 <- table(loan_data_test$loan_status,pred_co_015)
cm_015
```

 - Accuracy: `r sum(diag(cm_015))/sum(cm_015)`
 - Sensitivity: `r cm_015[2,2]/sum(cm_015[2,])`
 - Specificity: `r cm_015[1,1]/sum(cm_015[1,])`

General properties of credit risk model are that by increasing cutoff value

 - the accuracy increases,
 - the sensitivity decreases and
 - the specificity increases.

This phenomenon is purely due to the fact that defaults are usually rare events.

# 3. Decision Trees



